# Mamba Pretraining Configuration
# Anonymized for publication

# Default configuration (shared across all variants)
default: &default
  # === Data Paths (UPDATE THESE) ===
  data_root: /path/to/preprocessed/data  # Directory containing features_train, features_test
  stat_dir: /path/to/statistics          # Directory with normalization statistics
  checkpoint_dir: /path/to/checkpoints   # Where to save model checkpoints

  # === Data Configuration ===
  data_version: dataset_name
  n_data_idx: 10000000                   # Number of training samples
  limit_data: true
  limit_size: 10000000

  # === Training Configuration ===
  batch_size: 256                        # Global batch size (distributed across GPUs)
  local_batch_size: 16                   # Batch size per GPU (256 / 16 GPUs)
  valid_batch_size: 64
  local_valid_batch_size: 4

  # === Optimization ===
  max_lr: 0.0002
  min_lr: 0.00002
  warmup_steps: 1000
  total_steps: 50000

  # === Model Configuration ===
  base_dim: 256
  embed_dim: 256
  klen: 30                               # Sequence context length
  dropout: 0.1
  init_std: 0.025

  # === Embedding Configuration ===
  embed_method: additive                 # or 'concat'
  pe_method: nerf                        # Positional encoding: 'nerf' or 'sinusoidal'

  # === Training Settings ===
  num_data_workers: 8
  log_to_screen: true
  save_version: mamba_pretrain

# ============================================================================
# Mamba 5M Configuration (~4.6M parameters)
# ============================================================================
mamba_5m:
  <<: *default
  embed_dim: 256
  num_layers_backbone: 12
  d_state: 16                            # State space dimension
  d_conv: 4                              # Convolutional kernel size
  expand: 2                              # Expansion factor
  klen: 30
  batch_size: 256
  local_batch_size: 16                   # Adjust based on available GPUs
  valid_batch_size: 64
  local_valid_batch_size: 4
  warmup_steps: 1000
  total_steps: 50000
  max_lr: 0.0002
  min_lr: 0.00002
  dropout: 0.1
  save_version: mamba_5m

# ============================================================================
# Mamba2 5M Configuration (~5.1M parameters)
# ============================================================================
mamba2_5m:
  <<: *default
  embed_dim: 256
  num_layers_backbone: 12
  d_state: 128                           # Larger state space for Mamba2
  d_conv: 4
  expand: 2
  headdim: 64                            # Head dimension for Mamba2
  ngroups: 1                             # Number of groups
  klen: 30
  batch_size: 256
  local_batch_size: 16
  valid_batch_size: 64
  local_valid_batch_size: 4
  warmup_steps: 1000
  total_steps: 50000
  max_lr: 0.0002
  min_lr: 0.00002
  dropout: 0.1
  save_version: mamba2_5m

# ============================================================================
# Smaller Mamba Configuration for Testing (~1.5M parameters)
# ============================================================================
mamba_small:
  <<: *default
  embed_dim: 128
  num_layers_backbone: 6
  d_state: 16
  d_conv: 4
  expand: 2
  klen: 30
  batch_size: 128
  local_batch_size: 8
  valid_batch_size: 32
  local_valid_batch_size: 2
  warmup_steps: 500
  total_steps: 25000
  max_lr: 0.0002
  min_lr: 0.00002
  dropout: 0.1
  save_version: mamba_small
